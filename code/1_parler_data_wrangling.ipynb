{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from src.nielsen_helpers import get_s3_files, process_files, get_regex_domains\n",
    "from src.helpers import  plot_top, set_size\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy.random  as rnd\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall weights\n",
    "\n",
    "## Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users(df, reg):\n",
    "    tmp = df[df.url.apply(lambda x: reg.match(x) is not None)].copy(deep=True)\n",
    "    if len(tmp) == 0:\n",
    "        return pd.DataFrame()\n",
    "    tmp[\"platform\"] = tmp.url.apply(lambda x: \n",
    "                                    [y for y in reg.match(x).groups() if y is not None][-2])\n",
    "    tmp = tmp.groupby([\"nol_id\", \"platform\", pd.Grouper(key=\"activitydatetime\", freq=\"D\")])\\\n",
    "        .agg({\"viewduration\": sum, \"url\": len})\\\n",
    "        .reset_index()\n",
    "    tmp.activitydatetime = pd.to_datetime(tmp.activitydatetime)\n",
    "    return tmp\n",
    "\n",
    "recompute = False\n",
    "start_date, end_date = \"2020-05-01\", \"2021-07-01\"\n",
    "filesn, dem = get_s3_files(start_date, end_date)\n",
    "path = \"/data/deplatforming/data/tmp_wall/df_fringe_to_weight_{}_to_{}.csv\".format(start_date, end_date)\n",
    "regex_fringe = re.compile(get_regex_domains(path_to_regexes='./data/domains.csv', category_key='category', \n",
    "                                             category='fringe', domain_keys='domains'))\n",
    "\n",
    "if recompute:\n",
    "    df_fringe = process_files(filesn, start_date, end_date, num_workers=15, mf=get_users, \n",
    "                              args_mf=regex_fringe)\n",
    "    df_fringe = df_fringe.groupby([\"nol_id\", \"platform\", \"activitydatetime\"]).sum().reset_index()\n",
    "    df_fringe.to_csv(path, index=False)\n",
    "\n",
    "else:\n",
    "    df_fringe = pd.read_csv(path)\n",
    "    df_fringe.activitydatetime = pd.to_datetime(df_fringe.activitydatetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users(df, reg):\n",
    "    tmp = df[df.url.apply(lambda x: reg.match(x) is not None)].copy(deep=True)\n",
    "    if len(tmp) == 0:\n",
    "        return pd.DataFrame()\n",
    "    tmp[\"platform\"] = tmp.url.apply(lambda x: \n",
    "                                    [y for y in reg.match(x).groups() if y is not None][-2])\n",
    "    tmp = tmp.groupby([\"nol_id\", \"platform\", pd.Grouper(key=\"activitydatetime\", freq=\"D\")])\\\n",
    "        .agg({\"viewduration\": sum, \"url\": len})\\\n",
    "        .reset_index()\n",
    "    tmp.activitydatetime = pd.to_datetime(tmp.activitydatetime)\n",
    "    return tmp\n",
    "\n",
    "recompute = False\n",
    "start_date, end_date = \"2020-05-01\", \"2021-07-01\"\n",
    "filesn, dem = get_s3_files(start_date, end_date)\n",
    "path = \"/data/deplatforming/data/tmp_wall/df_mainstream_to_weight_{}_to_{}.csv\".format(start_date, end_date)\n",
    "regex_mainstream = re.compile(get_regex_domains(path_to_regexes='./data/domains.csv', category_key='category', \n",
    "                                             category='osn', domain_keys='domains'))\n",
    "if recompute:\n",
    "    df_mainstream = process_files(filesn, start_date, end_date, num_workers=15, mf=get_users, \n",
    "                              args_mf=regex_mainstream)\n",
    "    df_mainstream = df_mainstream.groupby([\"nol_id\", \"platform\", \"activitydatetime\"]).sum().reset_index()\n",
    "    df_mainstream.to_csv(path, index=False)\n",
    "\n",
    "else:\n",
    "    df_mainstream = pd.read_csv(path)\n",
    "    df_mainstream.activitydatetime = pd.to_datetime(df_mainstream.activitydatetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(df):\n",
    "    if len(df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    tmp = df.groupby([\"nol_id\", pd.Grouper(key=\"activitydatetime\", freq=\"D\")])\\\n",
    "        .agg({\"viewduration\": sum, \"url\": len})\\\n",
    "        .reset_index()\n",
    "    tmp.activitydatetime = pd.to_datetime(tmp.activitydatetime)\n",
    "    return tmp\n",
    "\n",
    "recompute = False\n",
    "start_date, end_date = \"2020-05-01\", \"2021-07-01\"\n",
    "filesn, dem = get_s3_files(start_date, end_date)\n",
    "path = \"/data/deplatforming/data/tmp_wall/df_all_to_weight_{}_to_{}.csv\".format(start_date, end_date)\n",
    "                                  \n",
    "if recompute:\n",
    "    df_all = process_files(filesn, start_date, end_date, num_workers=15, mf=get_all)\n",
    "    df_all = df_all.groupby([\"nol_id\", \"activitydatetime\"]).sum().reset_index()\n",
    "    df_all.to_csv(path, index=False)\n",
    "\n",
    "else:\n",
    "    df_all = pd.read_csv(path)\n",
    "    df_all.activitydatetime = pd.to_datetime(df_all.activitydatetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users(df, reg):\n",
    "    apps = [\"Rumble - Video Battles\", \"MeWe Network\", \"Parler\", \"DLive · Stream on Blockchain\",\n",
    "           \"Telegram Messenger\"]\n",
    "    tmp =  df[df.domain_name.apply(lambda x: not pd.isna(x) and reg.match(x) is not None) \n",
    "              | df.app_name.apply(lambda x: not pd.isna(x) and x in apps) ].copy(deep=True)\n",
    "    \n",
    "    tmp.loc[tmp.app_name == \"Rumble - Video Battles\", \"domain_name\"] = \"rumble.com\"\n",
    "    tmp.loc[tmp.app_name == \"MeWe Network\", \"domain_name\"] = \"mewe.com\"\n",
    "    tmp.loc[tmp.app_name == \"Parler\", \"domain_name\"] = \"parler.com\"\n",
    "    tmp.loc[tmp.app_name == \"DLive · Stream on Blockchain\", \"domain_name\"] = \"dlive.tv\"\n",
    "    tmp.loc[tmp.app_name == \"Telegram Messenger\", \"domain_name\"] = \"telegram.org\"\n",
    "    \n",
    "    tmp[\"is_app\"] = False\n",
    "    for appn in apps:\n",
    "        tmp.loc[tmp.app_name == appn, \"is_app\"] = True\n",
    " \n",
    "    tmp[\"platform\"] = tmp.domain_name.apply(lambda x: \n",
    "                                    [y for y in reg.match(x).groups() if y is not None][-2])\n",
    "\n",
    "    tmp = tmp.groupby([\"mobile_id\", \"platform\", pd.Grouper(key=\"activitydatetime\", freq=\"D\"), \"is_app\"])\\\n",
    "        .agg({\"duration\": sum, \"app_name\": len})\\\n",
    "        .reset_index()\n",
    "    tmp.activitydatetime = pd.to_datetime(tmp.activitydatetime)\n",
    "    return tmp\n",
    "    \n",
    "\n",
    "regex_fringe = re.compile(get_regex_domains(path_to_regexes='./data/domains.csv', category_key='category', \n",
    "                                             category='fringe', domain_keys='domains'))\n",
    "recompute = False\n",
    "start_date, end_date = \"2020-05-01\", \"2021-07-01\"\n",
    "filesn, ap, dem = get_s3_files(start_date, end_date, is_mobile=True,\n",
    "                                     pathv=\"s3://epfl-collaboration-paspkaoe1nx9ptad1k8rjh5maact6use1a-s3alias/tmp/\")\n",
    "path = \"/data/deplatforming/data/tmp_wall/df_fringe_to_weight_{}_to_{}_mob.csv\".format(start_date, end_date)\n",
    "\n",
    "if recompute:\n",
    "    df_fringe_mob = process_files(filesn, start_date, end_date, num_workers=15, mf=get_users, \n",
    "                              args_mf=regex_fringe, is_mobile=True)\n",
    "    df_fringe_mob = df_fringe_mob.groupby([\"mobile_id\", \"platform\", \"activitydatetime\", \"is_app\"])\\\n",
    "                                .sum().reset_index()\n",
    "    df_fringe_mob.to_csv(path, index=False)\n",
    "\n",
    "else:\n",
    "    df_fringe_mob = pd.read_csv(path)\n",
    "    df_fringe_mob.activitydatetime = pd.to_datetime(df_fringe_mob.activitydatetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users(df, reg):\n",
    "    apps = [\"Facebook\", \"Facebook Lite\", \"Twitter\", \"Pinterest\", \"Reddit: Trending News & Tips\",\n",
    "           \"Instagram\", \"YouTube: Watch, Listen, Stream\", \"TikTok\", \"TikTok - Make Your Day\",\n",
    "           \"LinkedIn: Network & Job Finder\", \"Snapchat\", 'WhatsApp Messenger',\n",
    "            \"Nextdoor: Local Neighborhood\"]\n",
    "    tmp =  df[df.domain_name.apply(lambda x: not pd.isna(x) and reg.match(x) is not None) \n",
    "              | df.app_name.apply(lambda x: not pd.isna(x) and x in apps) ].copy(deep=True)\n",
    "    \n",
    "\n",
    "    tmp.loc[tmp.app_name == \"Facebook\", \"domain_name\"] = \"facebook.com\"\n",
    "    tmp.loc[tmp.app_name == \"Facebook Lite\", \"domain_name\"] = \"facebook.com\"\n",
    "    tmp.loc[tmp.app_name == \"Twitter\", \"domain_name\"] = \"twitter.com\"\n",
    "    tmp.loc[tmp.app_name == \"Pinterest\", \"domain_name\"] = \"pinterest.com\"\n",
    "    tmp.loc[tmp.app_name == \"Reddit: Trending News & Tips\", \"domain_name\"] = \"reddit.com\"\n",
    "    tmp.loc[tmp.app_name == \"Instagram\", \"domain_name\"] = \"instagram.com\"\n",
    "    tmp.loc[tmp.app_name == \"YouTube: Watch, Listen, Stream\", \"domain_name\"] = \"youtube.com\"\n",
    "    tmp.loc[tmp.app_name == \"TikTok\", \"domain_name\"] = \"tiktok.com\"\n",
    "    tmp.loc[tmp.app_name == \"TikTok - Make Your Day\", \"domain_name\"] = \"tiktok.com\"\n",
    "    tmp.loc[tmp.app_name == \"LinkedIn: Network & Job Finder\", \"domain_name\"] = \"tiktok.com\"\n",
    "    tmp.loc[tmp.app_name == \"Snapchat\", \"domain_name\"] = \"snapchat.com\"\n",
    "    tmp.loc[tmp.app_name == 'WhatsApp Messenger', \"domain_name\"] = \"whatsapp.com\"\n",
    "    tmp.loc[tmp.app_name == \"Nextdoor: Local Neighborhood\", \"domain_name\"] = \"nextdoor.com\"\n",
    "\n",
    "\n",
    "    tmp[\"is_app\"] = False\n",
    "    for appn in apps:\n",
    "        tmp.loc[tmp.app_name == appn, \"is_app\"] = True\n",
    " \n",
    "    tmp[\"platform\"] = tmp.domain_name.apply(lambda x: \n",
    "                                    [y for y in reg.match(x).groups() if y is not None][-2])\n",
    "\n",
    "    tmp = tmp.groupby([\"mobile_id\", \"platform\", pd.Grouper(key=\"activitydatetime\", freq=\"D\"), \"is_app\"])\\\n",
    "        .agg({\"duration\": sum, \"app_name\": len})\\\n",
    "        .reset_index()\n",
    "    tmp.activitydatetime = pd.to_datetime(tmp.activitydatetime)\n",
    "    return tmp\n",
    "    \n",
    "\n",
    "regex_mainstream = re.compile(get_regex_domains(path_to_regexes='./data/domains.csv', category_key='category', \n",
    "                                             category='osn', domain_keys='domains'))\n",
    "recompute = False\n",
    "start_date, end_date = \"2020-05-01\", \"2021-07-01\"\n",
    "filesn, ap, dem = get_s3_files(start_date, end_date, is_mobile=True,\n",
    "                                     pathv=\"s3://epfl-collaboration-paspkaoe1nx9ptad1k8rjh5maact6use1a-s3alias/tmp/\")\n",
    "\n",
    "path = \"/data/deplatforming/data/tmp_wall/df_mainstream_to_weight_{}_to_{}_mob.csv\".format(start_date, end_date)\n",
    "\n",
    "if recompute:\n",
    "    df_mainstream_mob = process_files(filesn, start_date, end_date, num_workers=8, mf=get_users, \n",
    "                              args_mf=regex_mainstream, is_mobile=True)\n",
    "    df_mainstream_mob = df_mainstream_mob.groupby([\"mobile_id\", \"platform\", \"activitydatetime\", \"is_app\"])\\\n",
    "                            .sum().reset_index()\n",
    "    df_mainstream_mob.to_csv(path, index=False)\n",
    "\n",
    "else:\n",
    "    df_mainstream_mob = pd.read_csv(path)\n",
    "    df_mainstream_mob.activitydatetime = pd.to_datetime(df_mainstream_mob.activitydatetime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_users(df):\n",
    "\n",
    "    df[\"is_app\"] = df.domain_name.isna()\n",
    "\n",
    "    df = df.groupby([\"mobile_id\", pd.Grouper(key=\"activitydatetime\", freq=\"D\"), \"is_app\"])\\\n",
    "        .agg({\"duration\": sum, \"app_name\": len})\\\n",
    "        .reset_index()\n",
    "    df.activitydatetime = pd.to_datetime(df.activitydatetime)\n",
    "    return df\n",
    "    \n",
    "recompute = False\n",
    "start_date, end_date = \"2020-05-01\", \"2021-07-01\"\n",
    "filesn, ap, dem = get_s3_files(start_date, end_date, is_mobile=True,\n",
    "                                     pathv=\"s3://epfl-collaboration-paspkaoe1nx9ptad1k8rjh5maact6use1a-s3alias/tmp/\")\n",
    "path = \"/data/deplatforming/data/tmp_wall/df_all_to_weight_{}_to_{}_mob.csv\".format(start_date, end_date)\n",
    "\n",
    "if recompute:\n",
    "    df_all_mob = process_files(filesn, start_date, end_date, num_workers=8, mf=get_users, is_mobile=True)\n",
    "    df_all_mob = df_all_mob.groupby([\"mobile_id\", \"activitydatetime\", \"is_app\"]).sum().reset_index()\n",
    "    df_all_mob.to_csv(path, index=False)\n",
    "\n",
    "else:\n",
    "    df_all_mob = pd.read_csv(path)\n",
    "    df_all_mob.activitydatetime = pd.to_datetime(df_all_mob.activitydatetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_parler(df, reg):\n",
    "    return df[df.url.apply(lambda x: reg.match(x) is not None)]\n",
    "\n",
    "regex_parler =  re.compile(\"(http(s)?://(www\\.)?([a-zA-Z0-9\\-_]+\\.)?)?(parler)\\.(com)\")\n",
    "recompute = False\n",
    "start_date, end_date = \"2020-12-01\", \"2021-01-01\"\n",
    "filesn, dem = get_s3_files(start_date, end_date)\n",
    "path = \"/data/deplatforming/data/tmp_parler/df_parler2_{}_to_{}.csv\".format(start_date, end_date)\n",
    "\n",
    "if recompute:\n",
    "    df_parler = process_files(filesn, start_date, end_date, num_workers=4, \n",
    "mf=get_users_parler, args_mf=regex_parler)\n",
    "    df_parler.to_csv(path, index=False)\n",
    "\n",
    "else:\n",
    "    df_parler = pd.read_csv(path)\n",
    "    df_parler.activitydatetime = pd.to_datetime(df_parler.activitydatetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "def get_users_parler_all(df, u):\n",
    "    return df.loc[df.nol_id.apply(lambda x: x in u), [\"nol_id\", \"activitydatetime\", \"viewduration\", \"url\"]]\n",
    "\n",
    "recompute = False\n",
    "\n",
    "tmp = df_parler.groupby(\"nol_id\").viewduration.sum().sort_values() \n",
    "tmp = tmp[tmp > 180].index.values\n",
    "treated_users = set(tmp)\n",
    "print(len(treated_users))\n",
    "start_date, end_date = \"2020-12-01\", \"2021-06-01\"\n",
    "filesn, dem = get_s3_files(start_date, end_date)\n",
    "\n",
    "path = \"/data/deplatforming/data/tmp_parler/df_parler2_all_{}_to_{}.csv\".format(start_date, end_date)\n",
    "if recompute:\n",
    "    df_parler_all = process_files(filesn, start_date, end_date, num_workers=15, mf=get_users_parler_all, \n",
    "                                    args_mf=treated_users)\n",
    "    df_parler_all.to_csv(path, index=False)\n",
    "else:\n",
    "    df_parler_all = pd.read_csv(path)\n",
    "    df_parler_all.activitydatetime = pd.to_datetime(df_parler_all.activitydatetime)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_control(df, reg):\n",
    "    return df[df.url.apply(lambda x: reg.match(x) is not None)]\n",
    "\n",
    "recompute = False\n",
    "start_date, end_date = \"2020-12-01\", \"2021-01-01\"\n",
    "filesn, dem = get_s3_files(start_date, end_date)\n",
    "path = \"/data/deplatforming/data/tmp_parler/df_control2_{}_to_{}.csv\".format(start_date, end_date)\n",
    "regex_fringe = re.compile(get_regex_domains(path_to_regexes='./data/domains.csv', category_key='category', \n",
    "                                             category='fringe', domain_keys='domains'))\n",
    "\n",
    "if recompute:\n",
    "    df_control = process_files(filesn, start_date, end_date, num_workers=4, \n",
    "                               mf=get_users_control,args_mf=regex_fringe)\n",
    "    df_control.to_csv(path, index=False)\n",
    "\n",
    "else:\n",
    "    df_control = pd.read_csv(path)\n",
    "    df_control.activitydatetime = pd.to_datetime(df_control.activitydatetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_control_all(df, u):\n",
    "    return df.loc[df.nol_id.apply(lambda x: x in u), [\"nol_id\", \"activitydatetime\", \"viewduration\", \"url\"]]\n",
    "\n",
    "recompute = False\n",
    "\n",
    "tmp = df_control.groupby(\"nol_id\").viewduration.sum().sort_values() \n",
    "tmp = tmp[tmp > 180].index.values\n",
    "control_users = set(tmp) - treated_users\n",
    "\n",
    "start_date, end_date = \"2020-12-01\", \"2021-06-01\"\n",
    "filesn, dem  = get_s3_files(start_date, end_date)\n",
    "\n",
    "path = \"/data/deplatforming/data/tmp_parler/df_control2_all_{}_to_{}.csv\".format(start_date, end_date)\n",
    "if recompute:\n",
    "    df_control_all = process_files(filesn, \n",
    "                                   start_date, \n",
    "                                   end_date, \n",
    "                                   num_workers=15,\n",
    "                                   mf=get_users_control_all, \n",
    "                                   args_mf=control_users)\n",
    "    df_control_all.to_csv(path, index=False)\n",
    "else:\n",
    "    df_control_all = pd.read_csv(path)\n",
    "    df_control_all.activitydatetime = pd.to_datetime(df_control_all.activitydatetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users(df, reg):\n",
    "    apps = [\"Parler\"]\n",
    "    tmp =  df[df.domain_name.apply(lambda x: not pd.isna(x) and reg.match(x) is not None) \n",
    "              | df.app_name.apply(lambda x: not pd.isna(x) and x in apps) ].copy(deep=True)\n",
    "    \n",
    "    tmp.loc[tmp.app_name == \"Parler\", \"domain_name\"] = \"parler.com\"\n",
    "    \n",
    "    tmp[\"is_app\"] = False\n",
    "    for appn in apps:\n",
    "        tmp.loc[tmp.app_name == appn, \"is_app\"] = True\n",
    " \n",
    "    tmp[\"platform\"] = tmp.domain_name.apply(lambda x: \n",
    "                                    [y for y in reg.match(x).groups() if y is not None][-2])\n",
    "\n",
    "    tmp.activitydatetime = pd.to_datetime(tmp.activitydatetime)\n",
    "    return tmp\n",
    "    \n",
    "regex_parler =  re.compile(\"(http(s)?://(www\\.)?([a-zA-Z0-9\\-_]+\\.)?)?(parler)\\.(com)\")\n",
    "recompute = False\n",
    "start_date, end_date = \"2020-12-01\", \"2021-01-01\"\n",
    "filesn, ap, dem = get_s3_files(start_date, end_date, is_mobile=True,\n",
    "                                     pathv=\"s3://epfl-collaboration-paspkaoe1nx9ptad1k8rjh5maact6use1a-s3alias/tmp/\")\n",
    "\n",
    "path = \"/data/deplatforming/data/tmp_parler/df_parler2_{}_to_{}_mob.csv\".format(start_date, end_date)\n",
    "\n",
    "if recompute:\n",
    "    df_parler_mob = process_files(filesn, start_date, end_date, num_workers=3, mf=get_users, args_mf=regex_parler,\n",
    "                             is_mobile=True)\n",
    "    df_parler_mob.to_csv(path, index=False)\n",
    "\n",
    "else:\n",
    "    df_parler_mob = pd.read_csv(path)\n",
    "    df_parler_mob.activitydatetime = pd.to_datetime(df_parler_mob.activitydatetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n"
     ]
    }
   ],
   "source": [
    "def get_users(df, u):\n",
    "    return df.loc[df.mobile_id.apply(lambda x: x in u), [\"mobile_id\", \n",
    "                                                     \"activitydatetime\", \n",
    "                                                     \"duration\", \n",
    "                                                     \"app_name\", \"domain_name\"]]\n",
    "\n",
    "recompute = False\n",
    "tmp = df_parler_mob.groupby(\"mobile_id\").duration.sum().sort_values() \n",
    "tmp = tmp[tmp > 180].index.values\n",
    "treated_users_mob = set(tmp)\n",
    "print(len(treated_users_mob))\n",
    "\n",
    "start_date, end_date = \"2020-12-01\", \"2021-06-01\"\n",
    "filesn, ap, dem = get_s3_files(start_date, end_date, is_mobile=True,\n",
    "                                     pathv=\"s3://epfl-collaboration-paspkaoe1nx9ptad1k8rjh5maact6use1a-s3alias/tmp/\")\n",
    "\n",
    "path = \"/data/deplatforming/data/tmp_parler/df_parler2_all_{}_to_{}_mob.csv\".format(start_date, end_date)\n",
    "if recompute:\n",
    "    df_parler_all_mob = process_files(filesn, start_date, end_date, num_workers=15, mf=get_users, \n",
    "                                    args_mf=treated_users_mob, is_mobile=True)\n",
    "    df_parler_all_mob.to_csv(path, index=False)\n",
    "else:\n",
    "    df_parler_all_mob = pd.read_csv(path)\n",
    "    df_parler_all_mob.activitydatetime = pd.to_datetime(df_parler_all_mob.activitydatetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users(df, reg):\n",
    "    apps = [\"Rumble - Video Battles\", \"MeWe Network\", \"Parler\", \"DLive · Stream on Blockchain\",\n",
    "           \"Telegram Messenger\"]\n",
    "    tmp =  df[df.domain_name.apply(lambda x: not pd.isna(x) and reg.match(x) is not None) \n",
    "              | df.app_name.apply(lambda x: not pd.isna(x) and x in apps) ].copy(deep=True)\n",
    "    \n",
    "    tmp.loc[tmp.app_name == \"Rumble - Video Battles\", \"domain_name\"] = \"rumble.com\"\n",
    "    tmp.loc[tmp.app_name == \"MeWe Network\", \"domain_name\"] = \"mewe.com\"\n",
    "    tmp.loc[tmp.app_name == \"Parler\", \"domain_name\"] = \"parler.com\"\n",
    "    tmp.loc[tmp.app_name == \"DLive · Stream on Blockchain\", \"domain_name\"] = \"dlive.tv\"\n",
    "    tmp.loc[tmp.app_name == \"Telegram Messenger\", \"domain_name\"] = \"telegram.org\"\n",
    "    \n",
    "    tmp[\"is_app\"] = False\n",
    "    for appn in apps:\n",
    "        tmp.loc[tmp.app_name == appn, \"is_app\"] = True\n",
    " \n",
    "    tmp[\"platform\"] = tmp.domain_name.apply(lambda x: \n",
    "                                    [y for y in reg.match(x).groups() if y is not None][-2])\n",
    "    tmp.activitydatetime = pd.to_datetime(tmp.activitydatetime)\n",
    "    return tmp\n",
    "    \n",
    "regex_fringe = re.compile(get_regex_domains(path_to_regexes='./data/domains.csv', category_key='category', \n",
    "                                             category='fringe', domain_keys='domains'))\n",
    "recompute = False\n",
    "start_date, end_date = \"2020-12-01\", \"2021-01-01\"\n",
    "filesn, ap, dem = get_s3_files(start_date, end_date, is_mobile=True,\n",
    "                                     pathv=\"s3://epfl-collaboration-paspkaoe1nx9ptad1k8rjh5maact6use1a-s3alias/tmp/\")\n",
    "path = \"/data/deplatforming/data/tmp_parler/df_control2_{}_to_{}_mob.csv\".format(start_date, end_date)\n",
    "\n",
    "if recompute:\n",
    "    df_control_mob = process_files(filesn, start_date, end_date, num_workers=4, mf=get_users, args_mf=regex_fringe,\n",
    "                             is_mobile=True)\n",
    "    df_control_mob.to_csv(path, index=False)\n",
    "\n",
    "else:\n",
    "    df_control_mob = pd.read_csv(path)\n",
    "    df_control_mob.activitydatetime = pd.to_datetime(df_control_mob.activitydatetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659\n"
     ]
    }
   ],
   "source": [
    "def get_users(df, u):\n",
    "    return df.loc[df.mobile_id.apply(lambda x: x in u), [\"mobile_id\", \n",
    "                                                     \"activitydatetime\", \n",
    "                                                     \"duration\", \n",
    "                                                     \"app_name\", \"domain_name\"]]\n",
    "\n",
    "recompute = False\n",
    "\n",
    "tmp = df_control_mob.groupby(\"mobile_id\").duration.sum().sort_values() \n",
    "tmp = tmp[tmp > 180].index.values\n",
    "control_users_mob = set(tmp) - treated_users_mob\n",
    "print(len(control_users_mob))\n",
    "start_date, end_date = \"2020-12-01\", \"2021-06-01\"\n",
    "filesn, ap, dem = get_s3_files(start_date, end_date, is_mobile=True,\n",
    "                                     pathv=\"s3://epfl-collaboration-paspkaoe1nx9ptad1k8rjh5maact6use1a-s3alias/tmp/\")\n",
    "\n",
    "path = \"/data/deplatforming/data/tmp_parler/df_control2_all_{}_to_{}_mob.csv\".format(start_date, end_date)\n",
    "if recompute:\n",
    "    df_control_all_mob = process_files(filesn, start_date, end_date, num_workers=15, mf=get_users, \n",
    "                                    args_mf=control_users_mob, is_mobile=True)\n",
    "    df_control_all_mob.to_csv(path, index=False)\n",
    "else:\n",
    "    df_control_all_mob = pd.read_csv(path)\n",
    "    df_control_all_mob.activitydatetime = pd.to_datetime(df_control_all_mob.activitydatetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccceb64ce9d14c9ea5e100d73784af4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/5354709 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8037c9985ec467a9c49060fd117f7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/5265568 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Desktop\n",
    "\n",
    "import regex\n",
    "import multiprocessing as mp\n",
    "\n",
    "df_control_all[\"is_treat\"] = 0\n",
    "df_parler_all[\"is_treat\"] = 1\n",
    "\n",
    "df_all = pd.concat([df_parler_all, df_control_all]).reset_index(drop=True)\n",
    "\n",
    "def reg_group(series, reg):\n",
    "    tmp = reg.match(series)\n",
    "    \n",
    "    if tmp is None:\n",
    "        return None\n",
    "    \n",
    "    return [y for y in reg.match(series).groups() if y is not None][-2]\n",
    "    \n",
    "reg_fringe = re.compile(get_regex_domains(path_to_regexes='./data/domains.csv', \n",
    "                                          category_key='category', \n",
    "                                          category='fringe',\n",
    "                                          domain_keys='domains'))\n",
    "reg_osn = re.compile(get_regex_domains(path_to_regexes='./data/domains.csv', \n",
    "                                          category_key='category', \n",
    "                                          category='osn',\n",
    "                                          domain_keys='domains'))\n",
    "\n",
    "df_all[\"platform\"] = None\n",
    "\n",
    "df_all.loc[df_all.platform.isna(), \"platform\"] = df_all.loc[df_all.platform.isna(), \"url\"].swifter.apply(\n",
    "                                                    lambda x: reg_group(x, reg_fringe))\n",
    "\n",
    "\n",
    "df_all.loc[df_all.platform.isna(), \"platform\"] = df_all.loc[df_all.platform.isna(), \"url\"].swifter.apply(\n",
    "                                                    lambda x: reg_group(x, reg_osn))\n",
    "\n",
    "path = \"/data/deplatforming/data/df_parler2_final.csv.gz\"\n",
    "df_all.to_csv(path, index=False, compression=\"infer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282a9b6b4ccf4386a14a8b1db0d2eb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/6829657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de1026de0304b98bd4375f5ececfd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/6610412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mobile\n",
    "\n",
    "df_control_all_mob[\"is_treat\"] = 0\n",
    "df_parler_all_mob[\"is_treat\"] = 1\n",
    "\n",
    "df_all_mob = pd.concat([df_control_all_mob, df_parler_all_mob]).reset_index(drop=True)\n",
    "\n",
    "df_all_mob.loc[df_all_mob.app_name == \"Rumble - Video Battles\", \"domain_name\"] = \"rumble.com\"\n",
    "df_all_mob.loc[df_all_mob.app_name == \"MeWe Network\", \"domain_name\"] = \"mewe.com\"\n",
    "df_all_mob.loc[df_all_mob.app_name == \"Parler\", \"domain_name\"] = \"parler.com\"\n",
    "df_all_mob.loc[df_all_mob.app_name == \"DLive · Stream on Blockchain\", \"domain_name\"] = \"dlive.tv\"\n",
    "df_all_mob.loc[df_all_mob.app_name == \"Telegram Messenger\", \"domain_name\"] = \"telegram.org\"\n",
    "df_all_mob.loc[df_all_mob.app_name == \"Facebook\", \"domain_name\"] = \"facebook.com\"\n",
    "df_all_mob.loc[df_all_mob.app_name == \"Facebook Lite\", \"domain_name\"] = \"facebook.com\"\n",
    "df_all_mob.loc[df_all_mob.app_name == \"Twitter\", \"domain_name\"] = \"twitter.com\"\n",
    "df_all_mob.loc[df_all_mob.app_name == \"Reddit: Trending News & Tips\", \"domain_name\"] = \"reddit.com\"\n",
    "df_all_mob.loc[df_all_mob.app_name == \"Instagram\", \"domain_name\"] = \"instagram.com\"\n",
    "df_all_mob.loc[df_all_mob.app_name == \"YouTube: Watch, Listen, Stream\", \"domain_name\"] = \"youtube.com\"\n",
    "df_all_mob.loc[df_all_mob.app_name == \"TikTok\", \"domain_name\"] = \"tiktok.com\"\n",
    "df_all_mob.loc[df_all_mob.app_name == \"TikTok - Make Your Day\", \"domain_name\"] = \"tiktok.com\"\n",
    "\n",
    "def reg_group(series, reg):\n",
    "    tmp = reg.match(series)\n",
    "    \n",
    "    if tmp is None:\n",
    "        return None\n",
    "    \n",
    "    return [y for y in reg.match(series).groups() if y is not None][-2]\n",
    "    \n",
    "reg_fringe = re.compile(get_regex_domains(path_to_regexes='./data/domains.csv', \n",
    "                                          category_key='category', \n",
    "                                          category='fringe',\n",
    "                                          domain_keys='domains'))\n",
    "reg_osn = re.compile(get_regex_domains(path_to_regexes='./data/domains.csv', \n",
    "                                          category_key='category', \n",
    "                                          category='osn',\n",
    "                                          domain_keys='domains'))\n",
    "\n",
    "df_all_mob[\"platform\"] = None\n",
    "\n",
    "bool_filter = df_all_mob.platform.isna() & (~df_all_mob.domain_name.isna())\n",
    "df_all_mob.loc[bool_filter, \"platform\"] = df_all_mob.loc[bool_filter, \"domain_name\"].swifter.apply(\n",
    "                                                    lambda x: reg_group(x, reg_fringe))\n",
    "\n",
    "bool_filter = df_all_mob.platform.isna() & (~df_all_mob.domain_name.isna())\n",
    "df_all_mob.loc[bool_filter, \"platform\"] = df_all_mob.loc[bool_filter, \"domain_name\"].swifter.apply(\n",
    "                                                    lambda x: reg_group(x, reg_osn))\n",
    "\n",
    "path = \"/data/deplatforming/data/df_parler2_final_mob.csv.gz\"\n",
    "df_all_mob.to_csv(path, index=False, compression=\"infer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desktop = pd.read_csv(\"/data/deplatforming/data/df_parler2_final.csv.gz\")\n",
    "df_mobile = pd.read_csv(\"/data/deplatforming/data/df_parler2_final_mob.csv.gz\")\n",
    "df_desktop[\"activitydatetime\"] = pd.to_datetime(df_desktop[\"activitydatetime\"])\n",
    "df_mobile[\"activitydatetime\"] = pd.to_datetime(df_mobile[\"activitydatetime\"])\n",
    "\n",
    "df_desktop[\"app_name\"] = None\n",
    "df_desktop[\"is_mobile\"] = False\n",
    "df_desktop = df_desktop.loc[:, [\"nol_id\", \"activitydatetime\", \"viewduration\", \n",
    "              \"app_name\", \"url\", \"is_treat\", \"platform\", \"is_mobile\"]]\n",
    "df_mobile[\"is_mobile\"] = True\n",
    "df_mobile.columns = df_desktop.columns\n",
    "\n",
    "df_desktop.loc[df_desktop.platform.isna(), \"platform\"] = \"other\"\n",
    "df_mobile.loc[df_mobile.platform.isna(), \"platform\"] = \"other\"\n",
    "\n",
    "df_mobile_prep = df_mobile.groupby([\"nol_id\", \"is_treat\", \"platform\", \n",
    "                                    pd.Grouper(key=\"activitydatetime\", freq=\"D\")])\\\n",
    "        .agg({\"viewduration\": sum}).reset_index()\n",
    "\n",
    "df_desktop_prep = df_desktop.groupby([\"nol_id\", \"is_treat\", \"platform\", \n",
    "                                      pd.Grouper(key=\"activitydatetime\", freq=\"D\")])\\\n",
    "        .agg({\"viewduration\": sum}).reset_index()\n",
    "\n",
    "path = \"/data/deplatforming/data/df_parler2_final_prep.csv.gz\"\n",
    "df_desktop_prep.to_csv(path, index=False, compression=\"infer\")\n",
    "\n",
    "path = \"/data/deplatforming/data/df_parler2_final_mob_prep.csv.gz\"\n",
    "df_mobile_prep.to_csv(path, index=False, compression=\"infer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
